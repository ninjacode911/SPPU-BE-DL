{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install keras\n"
      ],
      "metadata": {
        "id": "tUvRXx-_IsUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iw1xMxz8QA6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # Adjusted import for Keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Adjusted import for Keras\n",
        "from tensorflow.keras.models import Sequential  # Adjusted import for Keras\n",
        "from tensorflow.keras.layers import Dense, Dropout  # Adjusted import for Keras\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "nltk.download('stopwords')\n",
        "tokenizer=Tokenizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "def clean_doc(doc, vocab):\n",
        "    tokens = doc.split()\n",
        "    table = str.maketrans('', '', punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    tokens = [word for word in tokens if word.isalpha()] #to remove tokens of sign\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "    doc = load_doc(filename)\n",
        "    tokens = clean_doc(doc, vocab)\n",
        "    vocab.update(tokens)\n",
        "\n",
        "def doc_to_line(filename, vocab):\n",
        "    doc = load_doc(filename)\n",
        "    tokens = clean_doc(doc, vocab)\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def process_docs(directory, vocab, is_train):\n",
        "    lines = list()\n",
        "    movie_reviews_path = nltk.data.find('corpora/movie_reviews').path\n",
        "    directory_path = os.path.join(movie_reviews_path, directory)\n",
        "    for filename in listdir(directory_path):\n",
        "        if is_train and filename.startswith('cv9'):\n",
        "            continue\n",
        "        if not is_train and not filename.startswith('cv9'):\n",
        "            continue\n",
        "        path = os.path.join(directory_path, filename)\n",
        "        line = doc_to_line(path, vocab)\n",
        "        lines.append(line)\n",
        "    return lines\n",
        "\n",
        "def process_docsl(directory,vocab):\n",
        "    movie_reviews_path = nltk.data.find('corpora/movie_reviews').path\n",
        "    directory_path = os.path.join(movie_reviews_path, directory)\n",
        "    for filename in listdir(directory_path):\n",
        "        if filename.startswith('cv9'):\n",
        "            continue\n",
        "        path = os.path.join(directory_path, filename)\n",
        "        add_doc_to_vocab(path, vocab)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rpJSIsyLHqZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "nltk.download('movie_reviews')"
      ],
      "metadata": {
        "id": "L2hqdP1rPImF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_ids = movie_reviews.fileids('pos')\n",
        "negavtive_ids = movie_reviews.fileids('neg')\n",
        "print(positive_ids)\n",
        "print(negavtive_ids)"
      ],
      "metadata": {
        "id": "9mwPDLTpM61p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Counter()\n",
        "process_docsl('neg', vocab)\n",
        "process_docsl('pos', vocab)\n",
        "print(len(vocab))\n",
        "print(vocab.most_common(50))"
      ],
      "metadata": {
        "id": "gMn0nhVLQHo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = None\n",
        "history=None\n",
        "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
        "    scores = list()\n",
        "    n_repeats = 1\n",
        "    n_words = Xtest.shape[1]\n",
        "    best_acc = 0\n",
        "\n",
        "    for i in range(n_repeats):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        history_temp=model.fit(Xtrain, ytrain, epochs=50, verbose=2)\n",
        "\n",
        "        loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "        scores.append(acc)\n",
        "        global history\n",
        "        global best_model\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_model = model  # Keep track of the best model\n",
        "            history=history_temp\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def prepare_data(train_docs, test_docs, mode):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(train_docs)  # Build the word index on the training data\n",
        "\n",
        "    # Convert the texts to matrix representation based on the specified mode\n",
        "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
        "    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
        "\n",
        "    # Convert to NumPy arrays (Keras works directly with NumPy)\n",
        "    Xtrain = np.array(Xtrain, dtype=np.float32)\n",
        "    Xtest = np.array(Xtest, dtype=np.float32)\n",
        "\n",
        "    return Xtrain, Xtest"
      ],
      "metadata": {
        "id": "IvQ16BmwZh_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_occurance=2\n",
        "tokens=[k for k,c in vocab.items() if c >= min_occurance]\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "id": "iLfw_nGbjrOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_list(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "save_list(tokens, 'vocab.txt')\n",
        "# save_list(test_tokens, 'test_vocab.txt')\n",
        "# save_list(tags, 'tags.txt')\n",
        "# save_list(test_tags, 'test_tags.txt')"
      ],
      "metadata": {
        "id": "Ong-SifD0oS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "print(len(vocab))\n",
        "print(vocab)\n"
      ],
      "metadata": {
        "id": "aWZMcQpo1E-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_lines = process_docs('pos', vocab, True)\n",
        "negative_lines = process_docs('neg', vocab, True)\n",
        "test_positive_lines = process_docs('pos', vocab, False)\n",
        "test_negative_lines = process_docs('neg', vocab, False)"
      ],
      "metadata": {
        "id": "fEAt3cpK1S8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_docs = negative_lines + positive_lines\n",
        "test_docs = test_negative_lines + test_positive_lines"
      ],
      "metadata": {
        "id": "FIi2ZXbo2HsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(train_docs))\n",
        "# print(len(test_docs))\n",
        "# positive_lines\n",
        "\n",
        "ytrain=np.array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
        "ytest=np.array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
        "result=pd.DataFrame()"
      ],
      "metadata": {
        "id": "MohdF3hd2O-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modes = ['binary']\n",
        "# modes = ['binary', 'count', 'tfidf', 'freq']\n",
        "\n",
        "\n",
        "for mode in modes:\n",
        "    Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
        "    score = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
        "    result[mode] = score\n",
        "\n",
        "    print('Mode:', mode)\n",
        "    print('Accuracy: %.3f (%.3f)' % (np.mean(score), np.std(score)))\n",
        "    print()\n",
        "\n",
        "print(result)\n",
        "result.boxplot()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OJMZPJfc3HSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict_sentiment(review, vocab, tokenizer, model):\n",
        "    # Clean the review text\n",
        "    tokens = clean_doc(review,vocab)\n",
        "\n",
        "    # Filter out tokens not in the vocabulary\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "\n",
        "    # Convert the filtered tokens into a string (sentence)\n",
        "    line = ' '.join(tokens)\n",
        "\n",
        "    # Encode the review text using the fitted tokenizer\n",
        "    encoded = tokenizer.texts_to_matrix([line], mode='binary')  # or 'freq', depending on your model\n",
        "\n",
        "    # Make a prediction using the trained model\n",
        "    yhat = model.predict(encoded, verbose=0)\n",
        "\n",
        "    # Round the prediction to get the sentiment\n",
        "    sentiment = 'positive' if yhat[0, 0] >= 0.5 else 'negative'\n",
        "    confidence = yhat[0, 0]\n",
        "\n",
        "    return sentiment, confidence\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qnOVA1w6E_R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "\n",
        "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
        "sentiment, confidence = predict_sentiment(text, vocab, tokenizer, best_model)\n",
        "print(f'Sentiment: {sentiment}, Confidence: {confidence:.6f}')\n",
        "\n",
        "# text = 'good better'\n",
        "text = 'bad movie ever!'\n",
        "sentiment, confidence = predict_sentiment(text, vocab, tokenizer, best_model)\n",
        "print(f'Sentiment: {sentiment}, Confidence: {confidence:.6f}')\n"
      ],
      "metadata": {
        "id": "2iwLV3RdK9HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model accuracy and loss')\n",
        "plt.ylabel('Accuracy / Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train accuracy', 'Train loss'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_2SMmruzrRFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.summary()\n"
      ],
      "metadata": {
        "id": "hPAdbl_hrp29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TiOZiiXHGADF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}